{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import graphviz\n",
    "from pandas import read_csv\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score,f1_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading data but unlike logistic regression we wont standardize the data since it wont affect decision tree classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=read_csv(\"../data/california_housing.csv\")\n",
    "x=data.drop(\"MedianHouseValue\",axis=1)\n",
    "priceColumn=data[\"MedianHouseValue\"]\n",
    "averagePrice=np.median(priceColumn)\n",
    "y=(priceColumn < averagePrice).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain,xTest,yTrain,yTest=train_test_split(x,y,random_state=42,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree results are highly sensitive to hyperparameters. To find the best configuration, we will employ `GridSearchCV` to evaluate all possible parameter combinations and select the optimal model based on precision, f1_score, and recallScore metrics, which were utilized in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best params : {'max_depth': 7, 'min_samples_leaf': 20}\n",
      "Precision is : 0.8188727042431919\n",
      "recallScore is : 0.8428943937418514\n",
      "F1Score is : 0.8307099261162866\n"
     ]
    }
   ],
   "source": [
    "hyperParams={\n",
    "    \"max_depth\":[3,5,7],\n",
    "    \"min_samples_leaf\":[10,20,50]\n",
    "}\n",
    "\n",
    "scorer = ['f1','recall','precision']\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "grid=GridSearchCV(model,hyperParams,cv=5,scoring=scorer,refit=\"f1\")\n",
    "grid.fit(xTrain,yTrain)\n",
    "pred=grid.predict(xTest)\n",
    "print(\"The best params : \"+str(grid.best_params_))\n",
    "precision = precision_score(yTest, pred)\n",
    "print(\"Precision is : \"+str(precision))\n",
    "recall = recall_score(yTest, pred)\n",
    "print(\"recallScore is : \"+str(recall))\n",
    "f1 = f1_score(yTest, pred)\n",
    "print(\"F1Score is : \"+str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results align closely with logistic regression. By fine-tuning hyperparameters, both models can achieve better performance. After experimenting with various max_depth values, a depth of 7 proved optimal, preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visulazing the tree with graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision tree.pdf'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = export_graphviz(grid.best_estimator_, out_file=None, feature_names=x.columns, class_names=[\"expensive\",\"affordable\"])\n",
    "graph=graphviz.Source(dot_data)\n",
    "graph.render(\"decision tree\",cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the generated tree we can deduce the following:\n",
    "\n",
    "1. `Median income`: Most important, splits the data at the root. Lower income leads to affordable housing prediction.\n",
    "2. `Average rooms`: Higher values often predict expensive housing.\n",
    "3. `Average occupancy`: Lower occupancy associates with affordable housing.\n",
    "4. `Location`: Significantly impacts affordability based on longitude and latitude.\n",
    "5. `House age`: Older houses are more likely to be affordable.\n",
    "6. `Population density`: May influence affordability.\n",
    "7. `Average bedrooms`: Plays a role in some cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
